---
title: "Electricity Volatility Quant Model"
format: html
editor: visual
---

##Preface

The following trading strategy is an extension of my previous project (in collaboration with Olivier Haley) regarding the use of batteries and their profitability on the Australian power grid. The purpose is to take our initial findings and take a different approach while applying structured quantitative processes to improve and enhance the results. I have opted to look at a modeling structure that has the ability to capture the power of volatility that occurs in pricing in order to capitalize on price movements at the optimal time. Our previous apporach relied solely on time of day. I now want to look at how I can use volatility analysis to potentially increase the profitability of the algorithm. For a detailed explanation of the Australian power grid, please refer to the [AEMC](https://www.aemc.gov.au/energy-system/electricity/electricity-system/NEM). This project will not go into as much depth explaining the grid system and pricing dynamics as our previous work. For Access to that information, please visit (https://github.com/tigerwoodsjr/cetf).

## Data Provided

CETF provided us with the following datasets:

-   Real-time 5 minute wholesale price data for 2015 - 2022.
-   Pre-dispatch prices for 2022. These prices are published up to 48 hours prior to the current time. These prices reflect expected bidding behaviour of generators in the market taking into account expected demand, temperatures, network constraints, etc.
-   Actual operational demand 2022. This value is only known after the demand has occurred and settled prices are published.
-   Forcasted Demand at the 10%, 50% and 90% confidence levels. These forcasted values take into account a multitude of factors such as weather, grid congestion and historical demand trends. 

## Rationale

The approach taken involves looking at how volatility in electricity prices varies throughout the day on the grid in New South Wales. This approach requires an understanding of the 'Bathtub Curve', or how prices follow a similar pattern during the day due to demand. This idea is depicted below: 

![Source: Kaixin Yu, Zhi Hern Tom, Lissy Xun, Haonan Zhong, Jiabao Zhang](photos/bathtub.png)


```{r, echo=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T)
```

```{r}
# Libraries
library(plotly)
library(tidyverse)
library(lubridate)
library(TTR)
library(ggplot2)
library(patchwork)
library(PerformanceAnalytics)
library(RColorBrewer)
library(cowplot)
library(forecast)
library(tseries)
library(zoo)
library(stochvol)
library(rugarch)
library(tsibble)
library(quantmod)
library(RTL)

```



```{r}
# data load-in
path <- "C:/Users/conno/OneDrive/Desktop/Winter 24/" # connor
# path <- "/Users/Olivier/4th year/" # olivier
nswprices2022 <- readr::read_csv(file =  paste0(path, "FAT/data/nswprices2022.csv"))
nswprices2023 <- readr::read_csv(file =  paste0(path, "FAT/data/nswprices2023.csv"))
all_state_raw <- readr::read_csv(file =  paste0(path, "FAT/data/AllStatesPrices.csv"))
predispatchNSW <- readr::read_csv(file =  paste0(path, "FAT/data/predispatchNSW.csv"))
# NSW5min <- readr::read_csv(file = paste0(path, "FAT/data/P5minute_solution_NSW.csv"))
actdemand2022_raw <- readr::read_csv(file =  paste0(path, "FAT/data/actdemand2022.csv"))
sourcegen <- readr::read_csv(file =  paste0(path, "FAT/data/NSWGenerationbysource.csv"))
fcastdemand <- readr::read_csv(file =  paste0(path, "FAT/data/fcdemandNSW.csv"))
```



```{r}
# Clean all state
all_state <- all_state_raw %>%
  filter(REGIONID == "NSW1") %>%
  transmute(DATETIME = date_time, ACTUAL_PRICE = RRP) %>% 
  mutate(LASTCHANGED = as.character(DATETIME)) %>%
  filter(!is.na(LASTCHANGED)) %>%
  mutate(times = as.numeric(gsub(":", "", substr(LASTCHANGED, 
                           nchar(LASTCHANGED) - 8, nchar(LASTCHANGED)-3))),
         times = replace_na(times, 0),
         year = as.numeric(format(DATETIME, "%Y")),
         month = as.numeric(format(DATETIME, "%m")), 
         week = as.numeric(format(DATETIME, "%W")),
         weekdate = as.numeric(format(DATETIME, "%u")),
         day = as.numeric(format(DATETIME, "%d")))

# Clean demand

actdemand2022 <- actdemand2022_raw %>%
  mutate(DATETIME = mdy_hm(INTERVAL_DATETIME), 
         date = as.Date(DATETIME))

# Clean price forcast

predispatch <- predispatchNSW %>% dplyr::select(DATETIME, 
                                                RRP)

# Clean demand forcast 

fcastdemand <- fcastdemand %>% mutate(DATETIME = INTERVAL_DATETIME) %>% 
  dplyr::select(DATETIME, OPERATIONAL_DEMAND_POE10, 
                                             OPERATIONAL_DEMAND_POE50,
                                             OPERATIONAL_DEMAND_POE90)

# combine forcasted demand and predispatch prices
fcast <- inner_join(predispatch, fcastdemand, by = "DATETIME")

# grab actual prices 
prices <- all_state %>% dplyr::select(DATETIME, ACTUAL_PRICE)

# Extrapolate 30 min forecast data to 5 minute intervals. 

fcast_expanded <- fcast %>%
  complete(DATETIME = seq(min(DATETIME), 
                                   max(DATETIME) + 25*60, 
                                   by = "5 min")) %>%
  fill(everything(), .direction = "downup") %>% 
   mutate(F_MA_24 = rollapply(RRP, width = 24, FUN = mean, align = "right", fill = NA)) %>% drop_na()
  

model_df <- all_state %>% 
  #larger than october 2021
  filter(DATETIME > as.POSIXct("2021-10-31 00:00:00"), DATETIME < as.POSIXct("2023-03-13 08:55:00")) %>% 
  mutate(P_MA_24 = rollapply(ACTUAL_PRICE, width = 24, FUN = mean, align = "right", fill = NA)) %>% 
  filter(times %% 5 == 0) %>% 
  na.omit()
#combine forecasts and actual prices, this is the df I will use from here forward.
model_df <- inner_join(model_df, fcast_expanded, by = "DATETIME")


#remove outliers 

model_df <- model_df %>%
  mutate(ACTUAL_PRICE = ifelse(abs(ACTUAL_PRICE - P_MA_24) > 2 * sd(ACTUAL_PRICE),
                                mean(ACTUAL_PRICE, na.rm = TRUE),
                                ACTUAL_PRICE),
         RRP = ifelse(abs(RRP - F_MA_24) > 2 * sd(RRP),
                                mean(RRP, na.rm = TRUE),
                                RRP))
```

Now lets look at some actual data that we have that shows how the price reflects the concept touched on in the image above. Here is a price plot from the same day of 4 months, each pertaining to a different season. The value for month is the respective month of the year.

```{r}
pdf <- model_df %>% filter(year == 2022 & month %in% c(1, 4, 7, 10) & day == 20) %>% dplyr::select(times, ACTUAL_PRICE, month) %>% 
  mutate(avgprice = TTR::SMA(ACTUAL_PRICE, n = 18),
                            month = as.factor(month))

pdf %>% group_by(month) %>% plot_ly(x = ~times, y = ~avgprice, color = ~month, type = 'scatter', mode = 'lines') %>% 
  layout(title = 'Daily Price by Season',
         legend = list(title = list(text = 'Month of Year')))
```

Note that the bathtub shape is present, but it is clearly more visable in July (winter in Australia). The prominence of the shape fades away as you approach summer. This highlights the idea that there is seasonality present in price volatility. Our previous model performed very well in the winter months, and this plot highlights the main reason why. 


To further look into this, I threw on a rolling average standard deviation, broken down by season: 

```{r}
 # Create a dataframe with prices and a rolling standard deviation. 
window_size <- 24
dat <- model_df %>% dplyr::select(DATETIME, ACTUAL_PRICE, times, year, month, week, weekdate, day) %>% 
  mutate(adjprice = ACTUAL_PRICE + 1001,
         change = ACTUAL_PRICE - lag(ACTUAL_PRICE),
         logchange = log(adjprice / lag(adjprice))) %>%
  mutate(rollavg = TTR::SMA(change, n = window_size)) %>% 
  mutate(rollsd = rollapply(change, width = window_size, FUN = sd, na.rm = T, align = 'right', fill = NA),
         rollsdlog = rollapply(logchange, width = window_size, FUN = sd, na.rm = T, align = 'right', fill = NA)) %>% 
  drop_na() %>% 
  mutate(season = case_when(month %in% c(12, 1, 2) ~ 'Summer',
                          month %in% c(3, 4, 5) ~ 'Fall', 
                          month %in% c(6, 7, 8) ~ 'Winter',
                          month %in% c(9, 10, 11) ~ 'Spring', 
                          TRUE ~ 'na'))


dat %>% group_by(times, season) %>% 
 summarise(meansd = mean(rollsd)) %>% ungroup() %>% 
  plot_ly(x = ~times, y = ~meansd, color = ~season, type = 'scatter', mode = 'lines')


  
  
 # group_by(times) %>% 
 # summarise(meansd = mean(rollsd))





```

Here again you can clearly see that there is seasonality in volatility. The winter season is highly volatile in comparison to the summer, but the idea that volatility increases during peak demand hours is very much still visable. There is also both weekly and daily seasonality present, which is highlighted in the chart below, generated from Facebooks Prophet Model:

```{r}
library(prophet)
prophet_df <- model_df %>% 
  select(DATETIME, ACTUAL_PRICE) %>% 
  rename(ds = DATETIME, y = ACTUAL_PRICE) 
m1 <- prophet(prophet_df, daily.seasonality = TRUE, weekly.seasonality = TRUE, yearly.seasonality = TRUE)
future <-prophet::make_future_dataframe(m1, periods = 105120, freq = 300, include_history = FALSE)
forecast <- predict(m1, future)
# ?plot.prophet

prophet::prophet_plot_components(m1, forecast)

  



```
Note the patterns here: 

- There is a clear annual seasonality effect between winter and summer. 
- Weekly seasonality sees fluctuations between weekdays and weekends. 
- Daily seasonality follows the bathtub curve. 

From a high level, these trends make sense if you think about them from the perspective of demand. The issue now becomes how to keep this seasonality in mind when producing trading signals. There will need to be variability in signal generation on a daily, weekly, and annual level. This was a downfall of the previous model, and an area that I am trying to improve on with this optimization model. The reason for performing this analysis was to drive home that point heading into the next step, which is figuring out how to use volatility to trade on the grid and optimize the usage of the battery. 

## Taking A Look At Volatility: Stochastic Time Series Volatility and GARCH models. 

Stochastic volatility models and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are particularly useful in analyzing electricity prices due to the inherently volatile nature of energy markets. These models excel at capturing the dynamic behavior of volatility through time.

Stochastic volatility models allow for a random process to govern the variance of price changes, which can capture the unpredictability and potential heavy tails often observed in electricity price distributions. They are flexible in modeling the volatility's evolution and can account for shocks that persist over time, which is crucial for markets that can be influenced by unpredictable events like changes in supply and demand, regulatory shifts, or varying weather conditions.

GARCH models, on the other hand, are adept at modeling the conditional variance based on past squared residuals and past variances, making them highly effective for time series where volatility clustering occurs. This is a common feature in electricity markets, where periods of high volatility are often followed by low volatility and low by high. This autoregressive approach helps in forecasting future volatility based on the observed patterns. 

I initially took stabs at stochastic modelling process both with use of a Hidden Markov (MwSM package) to implement any regime switching properties, as well as forcasting volatility using Bayesian implementations to model heteroskedasticity (Stochvol package). Below is the output of my estimated volatilizes from different posterior quantiles:

```{r}

modeldat <- model_df %>% mutate(Date = as.Date(DATETIME)) %>% 
                             dplyr::select(Date, DATETIME, ACTUAL_PRICE, times, year, month, week, weekdate, day)
# Normalized log returns
modeldat <- modeldat %>% mutate(nmrlzd_price = ACTUAL_PRICE + 1001,
                                logret = log(nmrlzd_price / lag(nmrlzd_price))) %>% drop_na()

ret <- stochvol::logret(modeldat$nmrlzd_price, demean = TRUE)
ret <- ret[1:300]

res <- stochvol::svsample(ret, priormu = c(-10, 1), priorphi = c(20, 1.1), priorsigma = 0.1, thin = 10)

# summary(res, showlatent = FALSE)

volplot(res, forecast = 100, dates = modeldat$times[seq_along(ret)])
```

The above plot depicts two things: 

- 1. It effectively shows me nothing and does a poor job of forecasting.
- 2. It made me come to the realization that I am missing key components in my statistical knowledge base to properly engineer the data before implementing these stochastic processes.

I ran into the same issue running my Markov Models. My feature engineering to set up the variables in such a way that allows the model to produce meaningful output was not sophisticated enough to suffice. This is due in part to the complexity of the problem I am trying to solve with electricity data, as well as the reasons highlighted in the following paragraph. 


Stochastic models are complex and can be challenging to set up due to their intrinsic reliance on random processes and the intricate mathematical frameworks they employ. They require a deep understanding of probability theory and stochastic calculus to accurately model the random behavior of variables over time. Setting up data for these models involves not just extensive preprocessing to ensure quality and relevance but also sophisticated techniques to estimate and calibrate the numerous parameters that govern the random processes. This is where I am severely lacking the capability at my current stage in personal development to accurately tackle something of this nature. Additionally, stochastic models must be carefully validated and their assumptions tested against empirical data, which can be a non-trivial, data-intensive endeavor.

The GARCH models were producing statistically insignificant outputs, and trying to use GARCH to forecast volatility given the nature of how prices change so drastically and frequently in my data led to implementation issues and feasibility problems that were outside the scope of my timeline to fix. 

The process of working through, attempting, and failing on the above models allowed me to grow drastically, and I look forward to successfully tackling them in the future. 

For now, I decided to pivot to looking at how I could use basic analysis on volatility to provide insight on ways to use it to generate trading signals. 


## Volatility Analysis

My return data was split into seasons (Winter, Spring, Summer, Fall), utilizing 2022 as the period. I am okay with running the model sub-setted like this because the data is so high frequency, and the grid itself switched to 5 minute pricing in late 2021. This period ensures there was no regulatory change and I have access to data from every season. 

I ran rolling standard deviations on an initial window size of 2 hours. The thought process here is the average rolling volatility will capture spikes as the window rolls into the high vol zones, and start to drop when it rolls into low vol zones. This parameter will be part of my optimization process. 

## Strategy and Signals 

My strategy will consist of charge and discharge signals based on the rolling volatility of relative price changes, which aligns with the analysis performed above. There are clearly periods of higher vol throughout the day when prices spike, which also vary in magnitude by season. This is why I will separately optimize the following parameters for each season: 

- Rolling standard deviation window size. 
- Percentile threshold to initiate buy and sell signals.

My approach will be to buy when volatility is low (as prices are generally low, seen in the plots above) and sell when volatility is high (as prices are generally higher during the high vol clusters). Where our previous model focused solely on average price over time to generate signals, I am now focusing on using volatility and differentiating by season.

## Constraints
To ensure proper implementation of the strategy, the following battery constraints need to be recognized in the algorithm:

-   Size: 5.00 MW
-   Power: 4 hour
-   Capacity: 20.00 MWh (Size\*Power)
-   Max ramp rate: 0.417 MW/5 minutes (discharge and recharge)
-   Maximum DOD (Depth of Discharge): 1.00
-   Minimum DOD (Depth of Discharge): 0.00
-   Maximum average cycles per day: 1.75 cycles (1 cycle equals 4 hours of discharge and 4 hours of discharge)
    -   Maximum Operating Hours: 14.00 hours
-   Ability to change status every 5 minutes: charge, discharge, idle.
-   Prices: NEM wholesale prices charged when charging from the NEM, and NEM prices are earned when discharging to the NEM.

```{r}
dat <- all_state %>% mutate(Date = as.Date(DATETIME),
                            change = ACTUAL_PRICE - lag(ACTUAL_PRICE),
                      season = case_when(month %in% c(12, 1, 2) ~ 'Summer',
                          month %in% c(3, 4, 5) ~ 'Fall', 
                          month %in% c(6, 7, 8) ~ 'Winter',
                          month %in% c(9, 10, 11) ~ 'Spring', 
                          TRUE ~ 'na'))
#subset data by season

gdat <- dat %>% 
  filter(Date >= '2022-01-01', Date <= '2022-12-31') %>% 
  dplyr::select(DATETIME, ACTUAL_PRICE, change, season)
winter <- gdat %>% filter(season %in% 'Winter')
summer <- gdat %>% filter(season %in% 'Summer')
fall <- gdat %>% filter(season %in% 'Fall')
spring <- gdat %>% filter(season %in% 'Spring')

winter <- winter %>% timetk::tk_xts(date_var = DATETIME) %>%
  timetk::tk_tbl(rename_index = "DATETIME") %>%
  dplyr::mutate(across(where(is.numeric), round, 2))

################################################# Failed Garch Code #######################################################################

# make xts objects for each season and return type

#logretwinter <- xts(winter$logchange, order.by = winter$DATETIME)
#logretsummer <- xts(summer$logchange, order.by = summer$DATETIME)
#logretfall <- xts(fall$logchange, order.by = fall$DATETIME)
#logretspring <- xts(spring$logchange, order.by = spring$DATETIME)

#retwinter <- xts(winter$change, order.by = winter$DATETIME)
#retsummer <- xts(summer$change, order.by = summer$DATETIME)
#retfall <- xts(fall$change, order.by = fall$DATETIME)
#retspring <- xts(spring$change, order.by = spring$DATETIME)


#initialize spec
#spec <- ugarchspec(variance.model = list(garchOrder = c(1,1)),
                   # mean.model = list(armaOrder = c(0,0), include.mean = FALSE), 
                  # distribution.model = "norm")

# Fit the garch model to all subsetted data
#logfitwinter <- ugarchfit(spec = spec, data = logretwinter)

#logwinterfcast <- ugarchforecast(logfitwinter, n.ahead = 1)

#logwintersig <- logwinterfcast@forecast$sigmaFor

#logfitwinter

#################################failed markov code (Not including the various levels of feature engineering I tried) ###################################################

# start to discretize states, calculate rolling vol.
# N <- 24
# model_df <- model_df %>% mutate(adj_prices = ACTUAL_PRICE + 1001,
                                #adj_fcast = RRP + 1001,
  # change = log(adj_prices / lag(adj_prices)),
               # fcast_change = log(adj_fcast / lag(adj_fcast))) 

#rearrange for better viewing 
# mark_df <- model_df %>% dplyr::select(DATETIME, ACTUAL_PRICE, change, fcast_change, RRP, OPERATIONAL_DEMAND_POE10, OPERATIONAL_DEMAND_POE50) %>% drop_na()

# mark_df <- mark_df %>% filter(fcast_change != 0)

# pricediff <- diff(log(model_df$adj_prices))
# allow markov to find switching probabilities. 
# model <- lm(pricediff ~ 1)

# summary(model)
# mfit <- msmFit(model, k = 2, sw = rep(TRUE, 2))

# summary(mfit)

# R squared was either 0 or the model didn't run on all attempts.
```



```{r}
# make a dataframe that is inclusive of the constraints. 
max_cycles <- 1.75
hr <- 4
time_cap <- hr*2*max_cycles # set @ 7 hours charge / discharge per unit
ramp_rate_MWH <- 5/60*5 # 0.417 MW/5 minutes
startSOC <- 0.001
maxSOC <- 5*hr # 5 MWh * 4hs = 20 MWh
minSOC <- startSOC
cyles_per <- max_cycles*1/168 # 168 5 min == 14 hours
```


```{r}
# Start the Quant model process 

#define strategy parameters

sd_window <- 24
sd_sell_percentile <- 0.75
sd_buy_percentile <- 0.25


with_sig <- winter %>%
  mutate(
    rollSd = rollapply(change, width = sd_window, FUN = sd, na.rm = T, align = 'right', fill = NA)
  ) %>%
  drop_na() %>%
  mutate(
    signal = case_when(
      rollSd < quantile(rollSd, probs = sd_buy_percentile, na.rm = TRUE) ~ 1,   # Buy signal
      rollSd > quantile(rollSd, probs = sd_sell_percentile, na.rm = TRUE) ~ -1, # Sell signal
      TRUE ~ 0 # No signal
    )
  )

# add in battery constraints 


# 

```





```{r}
#function attempt 1 

# Define your constants and initial conditions
max_cycles <- 1.75
hr <- 4
time_cap <- hr*2*max_cycles
ramp_rate_MWH <- 5/60*5 # 0.417 MW/5 minutes
startSOC <- 0.001
maxSOC <- 5*hr
minSOC <- startSOC
cycles_per <- max_cycles*1/168 # per 5-minute interval

```


```{r}
# function attempt 3
library(dplyr)
library(zoo)

battery_strategy <- function(df, sd_window, sd_buy_percentile, sd_sell_percentile, ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per) {
  
  # Calculate the rolling standard deviation and set buy/sell signals
  df <- df %>%
    mutate(rollSd = rollapply(change, width = sd_window, FUN = sd, na.rm = TRUE, align = 'right', fill = NA)) %>%
    mutate(rollSd = na.approx(rollSd, na.rm = FALSE)) %>% # Fill NA values after alignment
    mutate(
      signal = case_when(
        rollSd < quantile(rollSd, probs = sd_buy_percentile, na.rm = TRUE) ~ 1,   # Buy signal
        rollSd > quantile(rollSd, probs = sd_sell_percentile, na.rm = TRUE) ~ -1, # Sell signal
        TRUE ~ 0  # No signal
      )
    )

  # Initialize SOC and cycle count vectors
  df$SOC <- rep(startSOC, nrow(df))
  df$cycle_count <- rep(0, nrow(df))
  
  SOC <- startSOC
  cycle_count <- 0
  current_day <- as.Date(df$DATETIME[1])

  # Iterate over the rows and apply battery constraints
  for (i in 1:nrow(df)) {
    # Reset cycle count if a new day starts
    if (as.Date(df$DATETIME[i]) != current_day) {
      cycle_count <- 0
      current_day <- as.Date(df$DATETIME[i])
    }

    # Increase SOC for buy signal if battery is not full
    if (df$signal[i] == 1 && SOC < maxSOC && cycle_count < max_cycles) {
      SOC <- min(SOC + ramp_rate_MWH, maxSOC)
      cycle_count <- min(cycle_count + cycles_per, max_cycles)
    }
    # Decrease SOC for sell signal if battery is not empty
    else if (df$signal[i] == -1 && SOC > minSOC && cycle_count < max_cycles) {
      SOC <- max(SOC - ramp_rate_MWH, minSOC)
      cycle_count <- min(cycle_count + cycles_per, max_cycles)
    }

    # Update SOC and cycle count for the current row
    df$SOC[i] <- SOC
    df$cycle_count[i] <- cycle_count
  }

  return(df)
}


# final_data <- battery_strategy(winter, sd_window, sd_buy_percentile, sd_sell_percentile, ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per)

```


```{r}

profit_loss <- function(data = data, ramp_rate_MWH = ramp_rate_MWH) {

data <- data %>%
  mutate(is_full = SOC > 19.7,
         is_empty = SOC < 0.002,
         full = ifelse(is_full, 'Y', 'N'),
         empty = ifelse(is_empty, 'Y', 'N'),
         trade = case_when(
           signal == 1 & !is_full ~ 1,
           signal == -1 & is_empty ~ 0,
           signal == -1 & !is_empty ~ -1,
           signal == 1 & is_full ~ 0,
           TRUE ~ 0
         ),
         money_in = case_when(trade == -1 ~ ACTUAL_PRICE * ramp_rate_MWH,
                              TRUE ~ 0),
         money_out = case_when(trade == 1 ~ -ACTUAL_PRICE * ramp_rate_MWH,
                               TRUE ~ 0),
         pl = money_in + money_out,
         cumeq = cumsum(pl))

return(data)

}


# pl <- profit_loss(final_data, ramp_rate_MWH)

```

```{r}
# combine the strategy functions 
combined_strategy <- function(data, sd_window, sd_buy_percentile, sd_sell_percentile) {
  
  # Constants and initial conditions
  max_cycles <- 1.75
  hr <- 4
  ramp_rate_MWH <- 5 / 60 * 5 # 0.417 MW/5 minutes
  startSOC <- 0.001
  maxSOC <- 5 * hr # 5 MWh * 4hs = 20 MWh
  minSOC <- startSOC
  cycles_per <- max_cycles * 1 / 288 # Assuming 288 5-min intervals in a day

  # Apply the battery strategy
  strategy_data <- battery_strategy(
    data, sd_window, sd_buy_percentile, sd_sell_percentile,
    ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per
  )
  
  # Compute profit and loss
  pl_data <- profit_loss(strategy_data, ramp_rate_MWH)
  
  return(pl_data)
}

# Now call the combined function with your specific parameters
output_winter <- combined_strategy(winter, sd_window, sd_buy_percentile, sd_sell_percentile)


# optimization function


optimize <- function(data, sd_window, sd_buy_percentile, sd_sell_percentile) {
  
  # Constants and initial conditions
  max_cycles <- 1.75
  hr <- 4
  ramp_rate_MWH <- 5 / 60 * 5 # 0.417 MW/5 minutes
  startSOC <- 0.001
  maxSOC <- 5 * hr # 5 MWh * 4hs = 20 MWh
  minSOC <- startSOC
  cycles_per <- max_cycles * 1 / 288 # Assuming 288 5-min intervals in a day

  # Apply the battery strategy
  strategy_data <- battery_strategy(
    data, sd_window, sd_buy_percentile, sd_sell_percentile,
    ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per
  )
  
  # Compute profit and loss
  pl_data <- profit_loss(strategy_data, ramp_rate_MWH)
  
  total_cumeq <- pl_data %>% slice_tail(n = 1) %>% dplyr::select(cumeq)
  
  return(total_cumeq)
}

# opt <- optimize(winter, sd_window, sd_buy_percentile, sd_sell_percentile)
```

```{r}
# Now I can start to optimize. Initialize parameter dfs

out <- expand.grid(
  sd_window = seq(from = 10, to = 30, by = 2),
  sd_buy_percentile = seq(from = 0.60, to = 0.90, by = 0.05),
  sd_sell_percentile = seq(from = 0.15, to = 0.45, by = 0.05)
)
```

```{r}
# optimization for winter
library(foreach)
library(doParallel)

n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
res <- foreach(
  i = 1:nrow(out),
  .combine = "cbind",
  .packages = c(
    "tidyverse",
    "zoo",
    "TTR",
    "lubridate"
    )
) %dopar% {
  optimize(data = winter, out[i, "sd_window"], out[i, "sd_buy_percentile"], out["sd_sell_percentile"])
}
stopCluster(cl)

res <- tibble::as_tibble(t(res))
colnames(res) <- names(opt)
out <- cbind(out, res)

out %>%  
```




