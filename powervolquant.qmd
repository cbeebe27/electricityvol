---
title: "Electricity Volatility Quant Model"
author: "Connor Beebe"
date: "2024-04-10"
format: html
editor: visual
self-contained: true
---

## Preface

The following trading strategy is an extension of my previous project (in collaboration with Olivier Haley) regarding the use of batteries and their profitability on the Australian power grid. The purpose is to take our initial findings and take a different approach while applying structured quantitative processes to improve and enhance the results. I have opted to look at a modeling structure that has the ability to capture the power of volatility that occurs in pricing in order to capitalize on price movements at the optimal time. Our previous apporach relied solely on time of day. I now want to look at how I can use volatility analysis to potentially increase the profitability of the algorithm. For a detailed explanation of the Australian power grid, please refer to the [AEMC](https://www.aemc.gov.au/energy-system/electricity/electricity-system/NEM). This project will not go into as much depth explaining the grid system and pricing dynamics as our previous work. For access to that information, please visit [github](https://github.com/tigerwoodsjr/cetf).

## Data Provided

CETF provided us with the following datasets:

-   Real-time 5 minute wholesale price data for 2015 - 2022.
-   Pre-dispatch prices for 2022. These prices are published up to 48 hours prior to the current time. These prices reflect expected bidding behaviour of generators in the market taking into account expected demand, temperatures, network constraints, etc.
-   Actual operational demand 2022. This value is only known after the demand has occurred and settled prices are published.
-   Forcasted Demand at the 10%, 50% and 90% confidence levels. These forcasted values take into account a multitude of factors such as weather, grid congestion and historical demand trends. 

## Rationale

The approach taken involves looking at how volatility in electricity prices varies throughout the day on the grid in New South Wales. This approach requires an understanding of the 'Bathtub Curve', or how prices follow a similar pattern during the day due to demand. This idea is depicted below: 

![Source: Kaixin Yu, Zhi Hern Tom, Lissy Xun, Haonan Zhong, Jiabao Zhang](photos/bathtub.png)


```{r, echo=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T)
```

```{r}
# Libraries
library(plotly)
library(tidyverse)
library(lubridate)
library(TTR)
library(ggplot2)
library(patchwork)
library(PerformanceAnalytics)
library(RColorBrewer)
library(cowplot)
library(forecast)
library(tseries)
library(zoo)
library(stochvol)
library(rugarch)
library(tsibble)
library(quantmod)
library(RTL)
library(gt)

```



```{r}
# data load-in
path <- "C:/Users/conno/OneDrive/Desktop/Winter 24/" # connor
# path <- "/Users/Olivier/4th year/" # olivier
nswprices2022 <- readr::read_csv(file =  paste0(path, "FAT/data/nswprices2022.csv"))
nswprices2023 <- readr::read_csv(file =  paste0(path, "FAT/data/nswprices2023.csv"))
all_state_raw <- readr::read_csv(file =  paste0(path, "FAT/data/AllStatesPrices.csv"))
predispatchNSW <- readr::read_csv(file =  paste0(path, "FAT/data/predispatchNSW.csv"))
# NSW5min <- readr::read_csv(file = paste0(path, "FAT/data/P5minute_solution_NSW.csv"))
actdemand2022_raw <- readr::read_csv(file =  paste0(path, "FAT/data/actdemand2022.csv"))
sourcegen <- readr::read_csv(file =  paste0(path, "FAT/data/NSWGenerationbysource.csv"))
fcastdemand <- readr::read_csv(file =  paste0(path, "FAT/data/fcdemandNSW.csv"))
```



```{r}
# Clean all state
all_state <- all_state_raw %>%
  filter(REGIONID == "NSW1") %>%
  transmute(DATETIME = date_time, ACTUAL_PRICE = RRP) %>% 
  mutate(LASTCHANGED = as.character(DATETIME)) %>%
  filter(!is.na(LASTCHANGED)) %>%
  mutate(times = as.numeric(gsub(":", "", substr(LASTCHANGED, 
                           nchar(LASTCHANGED) - 8, nchar(LASTCHANGED)-3))),
         times = replace_na(times, 0),
         year = as.numeric(format(DATETIME, "%Y")),
         month = as.numeric(format(DATETIME, "%m")), 
         week = as.numeric(format(DATETIME, "%W")),
         weekdate = as.numeric(format(DATETIME, "%u")),
         day = as.numeric(format(DATETIME, "%d")))

# Clean demand

actdemand2022 <- actdemand2022_raw %>%
  mutate(DATETIME = mdy_hm(INTERVAL_DATETIME), 
         date = as.Date(DATETIME))

# Clean price forcast

predispatch <- predispatchNSW %>% dplyr::select(DATETIME, 
                                                RRP)

# Clean demand forcast 

fcastdemand <- fcastdemand %>% mutate(DATETIME = INTERVAL_DATETIME) %>% 
  dplyr::select(DATETIME, OPERATIONAL_DEMAND_POE10, 
                                             OPERATIONAL_DEMAND_POE50,
                                             OPERATIONAL_DEMAND_POE90)

# combine forcasted demand and predispatch prices
fcast <- inner_join(predispatch, fcastdemand, by = "DATETIME")

# grab actual prices 
prices <- all_state %>% dplyr::select(DATETIME, ACTUAL_PRICE)

# Extrapolate 30 min forecast data to 5 minute intervals. 

fcast_expanded <- fcast %>%
  complete(DATETIME = seq(min(DATETIME), 
                                   max(DATETIME) + 25*60, 
                                   by = "5 min")) %>%
  fill(everything(), .direction = "downup") %>% 
   mutate(F_MA_24 = rollapply(RRP, width = 24, FUN = mean, align = "right", fill = NA)) %>% drop_na()
  

model_df <- all_state %>% 
  #larger than october 2021
  filter(DATETIME > as.POSIXct("2021-10-31 00:00:00"), DATETIME < as.POSIXct("2023-03-13 08:55:00")) %>% 
  mutate(P_MA_24 = rollapply(ACTUAL_PRICE, width = 24, FUN = mean, align = "right", fill = NA)) %>% 
  filter(times %% 5 == 0) %>% 
  na.omit()
#combine forecasts and actual prices, this is the df I will use from here forward.
model_df <- inner_join(model_df, fcast_expanded, by = "DATETIME")


#remove outliers 

model_df <- model_df %>%
  mutate(ACTUAL_PRICE = ifelse(abs(ACTUAL_PRICE - P_MA_24) > 2 * sd(ACTUAL_PRICE),
                                mean(ACTUAL_PRICE, na.rm = TRUE),
                                ACTUAL_PRICE),
         RRP = ifelse(abs(RRP - F_MA_24) > 2 * sd(RRP),
                                mean(RRP, na.rm = TRUE),
                                RRP))
```

Now lets look at some actual data that we have that shows how the price reflects the concept touched on in the image above. Here is a price plot from the same day of 4 months, each pertaining to a different season. The value for month is the respective month of the year.

```{r}
pdf <- model_df %>% filter(year == 2022 & month %in% c(1, 4, 7, 10) & day == 20) %>% dplyr::select(times, ACTUAL_PRICE, month) %>% 
  mutate(avgprice = TTR::SMA(ACTUAL_PRICE, n = 18),
                            month = as.factor(month))

pdf %>% group_by(month) %>% plot_ly(x = ~times, y = ~avgprice, color = ~month, type = 'scatter', mode = 'lines') %>% 
  layout(title = 'Daily Price by Season',
         legend = list(title = list(text = 'Month of Year')),
         xaxis = list(title = ''))
```

Note that the bathtub shape is present, but it is clearly more visable in July (winter in Australia). The prominence of the shape fades away as you approach summer. This highlights the idea that there is seasonality present in price volatility. Our previous model performed very well in the winter months, and this plot highlights the main reason why. 


To further look into this, I threw on a rolling average standard deviation, broken down by season: 

```{r}
 # Create a dataframe with prices and a rolling standard deviation. 
window_size <- 24
dat <- model_df %>% dplyr::select(DATETIME, ACTUAL_PRICE, times, year, month, week, weekdate, day) %>% 
  mutate(adjprice = ACTUAL_PRICE + 1001,
         change = ACTUAL_PRICE - lag(ACTUAL_PRICE),
         logchange = log(adjprice / lag(adjprice))) %>%
  mutate(rollavg = TTR::SMA(change, n = window_size)) %>% 
  mutate(rollsd = rollapply(change, width = window_size, FUN = sd, na.rm = T, align = 'right', fill = NA),
         rollsdlog = rollapply(logchange, width = window_size, FUN = sd, na.rm = T, align = 'right', fill = NA)) %>% 
  drop_na() %>% 
  mutate(season = case_when(month %in% c(12, 1, 2) ~ 'Summer',
                          month %in% c(3, 4, 5) ~ 'Fall', 
                          month %in% c(6, 7, 8) ~ 'Winter',
                          month %in% c(9, 10, 11) ~ 'Spring', 
                          TRUE ~ 'na'))


dat %>% group_by(times, season) %>% 
 summarise(meansd = mean(rollsd)) %>% ungroup() %>% 
  plot_ly(x = ~times, y = ~meansd, color = ~season, type = 'scatter', mode = 'lines') %>% layout(xaxis = list(title = ''))


  
  
 # group_by(times) %>% 
 # summarise(meansd = mean(rollsd))





```

Here again you can clearly see that there is seasonality in volatility. The winter season is highly volatile in comparison to the summer, but the idea that volatility increases during peak demand hours is very much still visable. There is also both weekly and daily seasonality present, which is highlighted in the chart below, generated from Facebooks Prophet Model:

```{r}
library(prophet)
prophet_df <- model_df %>% 
  dplyr::select(DATETIME, ACTUAL_PRICE) %>% 
  rename(ds = DATETIME, y = ACTUAL_PRICE) 
m1 <- prophet(prophet_df, daily.seasonality = TRUE, weekly.seasonality = TRUE, yearly.seasonality = TRUE)
future <-prophet::make_future_dataframe(m1, periods = 105120, freq = 300, include_history = FALSE)
forecast <- predict(m1, future)
# ?plot.prophet

prophet::prophet_plot_components(m1, forecast)

  



```
Note the patterns here: 

- There is a clear annual seasonality effect between winter and summer. 
- Weekly seasonality sees fluctuations between weekdays and weekends. 
- Daily seasonality follows the bathtub curve. 

From a high level, these trends make sense if you think about them from the perspective of demand. The issue now becomes how to keep this seasonality in mind when producing trading signals. There will need to be variability in signal generation on a daily, weekly, and annual level. This was a downfall of the previous model, and an area that I am trying to improve on with this optimization model. The reason for performing this analysis was to drive home that point heading into the next step, which is figuring out how to use volatility to trade on the grid and optimize the usage of the battery. 

## Taking A Look At Volatility: Stochastic Time Series Volatility and GARCH models. 

Stochastic volatility models and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are particularly useful in analyzing electricity prices due to the inherently volatile nature of energy markets. These models excel at capturing the dynamic behavior of volatility through time.

Stochastic volatility models allow for a random process to govern the variance of price changes, which can capture the unpredictability and potential heavy tails often observed in electricity price distributions. They are flexible in modeling the volatility's evolution and can account for shocks that persist over time, which is crucial for markets that can be influenced by unpredictable events like changes in supply and demand, regulatory shifts, or varying weather conditions.

GARCH models, on the other hand, are adept at modeling the conditional variance based on past squared residuals and past variances, making them highly effective for time series where volatility clustering occurs. This is a common feature in electricity markets, where periods of high volatility are often followed by low volatility and low by high. This autoregressive approach helps in forecasting future volatility based on the observed patterns. 

I initially took stabs at stochastic modelling process both with use of a Hidden Markov (MwSM package) to implement any regime switching properties, as well as forecasting volatility using Bayesian implementations to model heteroskedasticity (Stochvol package). Below is the output of my estimated volatilities from different posterior quantiles, with a forecast tagged on the front:

```{r}

modeldat <- model_df %>% mutate(Date = as.Date(DATETIME)) %>% 
                             dplyr::select(Date, DATETIME, ACTUAL_PRICE, times, year, month, week, weekdate, day)
# Normalized log returns
modeldat <- modeldat %>% mutate(nmrlzd_price = ACTUAL_PRICE + 1001,
                                logret = log(nmrlzd_price / lag(nmrlzd_price))) %>% drop_na()

ret <- stochvol::logret(modeldat$nmrlzd_price, demean = TRUE)
ret <- ret[1:300]

res <- stochvol::svsample(ret, priormu = c(-10, 1), priorphi = c(20, 1.1), priorsigma = 0.1, thin = 10)

# summary(res, showlatent = FALSE)

volplot(res, forecast = 100, dates = modeldat$times[seq_along(ret)])
```

The above plot depicts two things: 

- 1. It effectively shows me nothing and does a poor job of forecasting.
- 2. It made me come to the realization that I am missing key components in my statistical knowledge base to properly engineer the data before implementing these stochastic processes.

I ran into the same issue running my Markov Models. My feature engineering to set up the variables in such a way that allows the model to produce meaningful output was not sophisticated enough to suffice. This is due in part to the complexity of the problem I am trying to solve with electricity data, as well as the reasons highlighted in the following paragraph. 


Stochastic models are complex and can be challenging to set up due to their intrinsic reliance on random processes and the intricate mathematical frameworks they employ. They require a deep understanding of probability theory and stochastic calculus to accurately model the random behavior of variables over time. Setting up data for these models involves not just extensive preprocessing to ensure quality and relevance but also sophisticated techniques to estimate and calibrate the numerous parameters that govern the random processes. This is where I am severely lacking the capability at my current stage in personal development to accurately tackle something of this nature. Additionally, stochastic models must be carefully validated and their assumptions tested against empirical data, which can be a non-trivial, data-intensive endeavor.

The GARCH models were producing statistically insignificant outputs, and trying to use GARCH to forecast volatility given the nature of how prices change so drastically and frequently in my data led to implementation issues and feasibility problems that were outside the scope of my timeline to fix. 

The process of working through, attempting, and failing on the above models allowed me to grow drastically, and I look forward to successfully tackling them in the future. 

This led me to come up with an idea, that quite frankly I know won't be very successful, but it allows me to explore the answers to these questions: 

- **Why are industry professionals using stochastic models for battery optimization?**

- **What are the things to look for in a basic volatility model that could be improved by a stochastic process?**

I went back to the drawing board to try and find answers. Moving forward, I made sure that my model relied on the following: 

- I know the grid switches from high vol states to low vol states.
- I know there is seasonality and this will have an effect on these states, their magnitude, and the probability of them switching.
- My results, if broken down and optimized differing by season and relying solely on volatility signals, should produce different results for each season and (consequently due to my analysis lacking regime switching probabilities and diffusion processes) produce better results in high vol states and lackluster results in low vol states. 

The rest of the document and the model that follows aims to provide insight into these points.


## Volatility Analysis

My return data was split into seasons (Winter, Spring, Summer, Fall), utilizing 2022 as the period. I am okay with running the model sub-setted like this because the data is so high frequency, and the grid itself switched to 5 minute pricing in late 2021. This period ensures there was no regulatory change and I have access to data from every season. 

I ran rolling standard deviations on an initial window size of 2 hours. The thought process here is the average rolling volatility will capture spikes as the window rolls into the high vol zones, and start to drop when it rolls into low vol zones. This parameter will be part of my optimization process. This is in essence trying to capture what the stochastic models are made for: To detect when the market switches regimes from times of high volatility to low volatility. In abcence of a working stochastic model, this was the approach I came up with to get a working analysis that I can iterate over and optimize the parameters for. 

## Strategy and Signals 

My strategy will consist of charge and discharge signals based on the rolling volatility of relative price changes, which aligns with the analysis and visuals above. There are clearly periods of higher vol throughout the day when prices spike, which also vary in magnitude by season. This is why I will separately optimize the following parameters for each season: 

- Rolling standard deviation window size. 
- Percentile threshold to initiate buy and sell signals.
- Optimize parameters separately for each season.

My approach will be to buy when volatility is low (as prices are generally low, seen in the plots above) and sell when volatility is high (as prices are generally higher during the high vol clusters). Where our previous model focused solely on average price over time to generate signals, I am now focusing on using volatility and differentiating by season.

## Constraints
To ensure proper implementation of the strategy, the following battery constraints need to be recognized in the algorithm:

-   Size: 5.00 MW
-   Power: 4 hour
-   Capacity: 20.00 MWh (Size\*Power)
-   Max ramp rate: 0.417 MW/5 minutes (discharge and recharge)
-   Maximum DOD (Depth of Discharge): 1.00
-   Minimum DOD (Depth of Discharge): 0.00
-   Maximum average cycles per day: 1.75 cycles (1 cycle equals 4 hours of discharge and 4 hours of discharge)
-   Maximum Operating Hours: 14.00 hours
-   Ability to change status every 5 minutes: charge, discharge, idle.
-   Prices: NEM wholesale prices charged when charging from the NEM, and NEM prices are earned when discharging to the NEM.

## Strategy 

The strategy implemented is a quantitative trading approach that capitalizes on the fluctuating electrcity market by leveraging the storage capabilities of the battery. The core of the strategy revolves around the concept of buying and storing energy when prices are lower and selling energy when prices are higher, effectively attempting to 'buy low, sell high' on a granular, intra-day timescale.

The operational logic of the strategy is governed by a predictive model that utilizes the rolling standard deviation of energy price changes—a measure of market volatility—as a signal for trading opportunities. When market volatility is below a specified percentile threshold, the model perceives this as a stable, low-price opportunity to charge the battery (buy energy). Conversely, when volatility is above a higher percentile threshold, it interprets this as a volatile, high-price scenario to discharge the battery (sell energy). The strategy ensures that the battery operates within its physical and operational constraints, notably the maximum state of charge (SOC), the minimum SOC, and the maximum number of charge-discharge cycles allowed per day.

Optimization of the strategy is multi-faceted, focusing primarily on the parameters that define the volatility thresholds for buying and selling. The objective is to fine-tune these thresholds to maximize return while adhering to the battery's cycle considerations and the operational limitations imposed by the trading intervals. By systematically adjusting the standard deviation windows and quantile thresholds, the strategy seeks to identify the optimal balance between responsiveness to market conditions and the constraints of the battery system, enhancing profitability while remaining within the constraints of the trading operations. The outcome of this optimization process is intended to refine the model for performance across all four seasons, as optimization has been subset and ran for winter, summer, spring and fall. This means the model will be trained four different times to optimize the parameters across the seasons, and then tested on the corresponding season within the testing data. 

### Training and Testing Data
The model will be ran on the following timelines for training and testing:

- Training: 2022 (seasons trained separately)
- Testing: 2023 (seasons tested separately)

Australia is in the southern hemisphere, so the seasons are broken down as follows:

 - Summer (December, January, February)
 - Fall (March, April, May)
 - Winter (June, July, August)
 - Spring (September, October, November)

Cumulative equity will be recalculated when the data is combined together. The reason for this is due to the seasons overlapping with what would normally be a change in the year. Cumulative equity will be calculated as if the strategy were to start January 1st for both the training and testing sets. 


```{r}
dat <- all_state %>% mutate(Date = as.Date(DATETIME),
                            change = ACTUAL_PRICE - lag(ACTUAL_PRICE),
                      season = case_when(month %in% c(12, 1, 2) ~ 'Summer',
                          month %in% c(3, 4, 5) ~ 'Fall', 
                          month %in% c(6, 7, 8) ~ 'Winter',
                          month %in% c(9, 10, 11) ~ 'Spring', 
                          TRUE ~ 'na'))
#subset data by season

gdat <- dat %>% 
  filter(Date >= '2022-01-01', Date <= '2022-12-31') %>% 
  dplyr::select(DATETIME, ACTUAL_PRICE, change, season) 
winter <- gdat %>% filter(season %in% 'Winter') %>% mutate(month = month(DATETIME))
summer <- gdat %>% filter(season %in% 'Summer') %>% mutate(month = month(DATETIME),
                                                           ord = case_when(month == 12 ~ 1,
                                                                           month == 1 ~ 2,
                                                                           TRUE ~ 3)) %>% arrange(ord) %>% select(-ord)
fall <- gdat %>% filter(season %in% 'Fall') %>% mutate(month = month(DATETIME))
spring <- gdat %>% filter(season %in% 'Spring') %>% mutate(month = month(DATETIME))


################################################# Failed Garch Code #######################################################################

# make xts objects for each season and return type

#logretwinter <- xts(winter$logchange, order.by = winter$DATETIME)
#logretsummer <- xts(summer$logchange, order.by = summer$DATETIME)
#logretfall <- xts(fall$logchange, order.by = fall$DATETIME)
#logretspring <- xts(spring$logchange, order.by = spring$DATETIME)

#retwinter <- xts(winter$change, order.by = winter$DATETIME)
#retsummer <- xts(summer$change, order.by = summer$DATETIME)
#retfall <- xts(fall$change, order.by = fall$DATETIME)
#retspring <- xts(spring$change, order.by = spring$DATETIME)


#initialize spec
#spec <- ugarchspec(variance.model = list(garchOrder = c(1,1)),
                   # mean.model = list(armaOrder = c(0,0), include.mean = FALSE), 
                  # distribution.model = "norm")

# Fit the garch model to all subsetted data
#logfitwinter <- ugarchfit(spec = spec, data = logretwinter)

#logwinterfcast <- ugarchforecast(logfitwinter, n.ahead = 1)

#logwintersig <- logwinterfcast@forecast$sigmaFor

#logfitwinter

#################################failed markov code (Not including the various levels of feature engineering I tried) ###################################################

# start to discretize states, calculate rolling vol.
# N <- 24
# model_df <- model_df %>% mutate(adj_prices = ACTUAL_PRICE + 1001,
                                #adj_fcast = RRP + 1001,
  # change = log(adj_prices / lag(adj_prices)),
               # fcast_change = log(adj_fcast / lag(adj_fcast))) 

#rearrange for better viewing 
# mark_df <- model_df %>% dplyr::select(DATETIME, ACTUAL_PRICE, change, fcast_change, RRP, OPERATIONAL_DEMAND_POE10, OPERATIONAL_DEMAND_POE50) %>% drop_na()

# mark_df <- mark_df %>% filter(fcast_change != 0)

# pricediff <- diff(log(model_df$adj_prices))
# allow markov to find switching probabilities. 
# model <- lm(pricediff ~ 1)

# summary(model)
# mfit <- msmFit(model, k = 2, sw = rep(TRUE, 2))

# summary(mfit)

# R squared was either 0 or the model didn't run on all other attempts.
```



```{r}
# make a dataframe that is inclusive of the constraints. 
max_cycles <- 1.75
hr <- 4
time_cap <- hr*2*max_cycles # set @ 7 hours charge / discharge per unit
ramp_rate_MWH <- 5/60*5 # 0.417 MW/5 minutes
startSOC <- 0.001
maxSOC <- 5*hr # 5 MWh * 4hs = 20 MWh
minSOC <- startSOC
cyles_per <- max_cycles*1/168 # 168 5 min == 14 hours
```


```{r}
# Start the Quant model process 

#define strategy parameters

sd_window <- 24
sd_sell_percentile <- 0.75
sd_buy_percentile <- 0.25


with_sig <- winter %>%
  mutate(
    rollSd = rollapply(change, width = sd_window, FUN = sd, na.rm = T, align = 'right', fill = NA)
  ) %>%
  drop_na() %>%
  mutate(
    signal = case_when(
      rollSd < quantile(rollSd, probs = sd_buy_percentile, na.rm = TRUE) ~ 1,   # Buy signal
      rollSd > quantile(rollSd, probs = sd_sell_percentile, na.rm = TRUE) ~ -1, # Sell signal
      TRUE ~ 0 # No signal
    )
  )

# add in battery constraints 


# 

```





```{r}
#function attempt 1 


max_cycles <- 1.75
hr <- 4
time_cap <- hr*2*max_cycles
ramp_rate_MWH <- 5/60*5 # 0.417 MW/5 minutes
startSOC <- 0.001
maxSOC <- 5*hr
minSOC <- startSOC
cycles_per <- max_cycles*1/168 # per 5-minute interval

```


```{r}
# function attempt 3
library(dplyr)
library(zoo)

battery_strategy <- function(df, sd_window, sd_buy_percentile, sd_sell_percentile, ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per) {
  
  # Calculate the rolling standard deviation and set buy/sell signals
  df <- df %>%
    mutate(rollSd = rollapply(change, width = sd_window, FUN = sd, na.rm = TRUE, align = 'right', fill = NA)) %>%
    mutate(rollSd = na.approx(rollSd, na.rm = FALSE)) %>% # Fill NA values after alignment
    mutate(
      signal = case_when(
        rollSd < quantile(rollSd, probs = sd_buy_percentile, na.rm = TRUE) ~ 1,   # Buy signal
        rollSd > quantile(rollSd, probs = sd_sell_percentile, na.rm = TRUE) ~ -1, # Sell signal
        TRUE ~ 0  # No signal
      )
    )

  # Initialize SOC and cycle count vectors
  df$SOC <- rep(startSOC, nrow(df))
  df$cycle_count <- rep(0, nrow(df))
  
  SOC <- startSOC
  cycle_count <- 0
  current_day <- as.Date(df$DATETIME[1])

  # Iterate over the rows and apply battery constraints
  for (i in 1:nrow(df)) {
    # Reset cycle count 
    if (as.Date(df$DATETIME[i]) != current_day) {
      cycle_count <- 0
      current_day <- as.Date(df$DATETIME[i])
    }

    # Increase SOC for buy signal if battery not full
    if (df$signal[i] == 1 && SOC < maxSOC && cycle_count < max_cycles) {
      SOC <- min(SOC + ramp_rate_MWH, maxSOC)
      cycle_count <- min(cycle_count + cycles_per, max_cycles)
    }
    # Decrease SOC for sell signal if battery is not empty
    else if (df$signal[i] == -1 && SOC > minSOC && cycle_count < max_cycles) {
      SOC <- max(SOC - ramp_rate_MWH, minSOC)
      cycle_count <- min(cycle_count + cycles_per, max_cycles)
    }

    # Update SOC and cycle count for the current row
    df$SOC[i] <- SOC
    df$cycle_count[i] <- cycle_count
  }

  return(df)
}


# final_data <- battery_strategy(winter, sd_window, sd_buy_percentile, sd_sell_percentile, ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per)

```


```{r}

profit_loss <- function(data = data, ramp_rate_MWH = ramp_rate_MWH) {

data <- data %>%
  mutate(is_full = SOC > 19.7,
         is_empty = SOC < 0.002,
         full = ifelse(is_full, 'Y', 'N'),
         empty = ifelse(is_empty, 'Y', 'N'),
         trade = case_when(
           signal == 1 & !is_full ~ 1,
           signal == -1 & is_empty ~ 0,
           signal == -1 & !is_empty ~ -1,
           signal == 1 & is_full ~ 0,
           TRUE ~ 0
         ),
         money_in = case_when(trade == -1 ~ ACTUAL_PRICE * ramp_rate_MWH,
                              TRUE ~ 0),
         money_out = case_when(trade == 1 ~ -ACTUAL_PRICE * ramp_rate_MWH,
                               TRUE ~ 0),
         pl = money_in + money_out,
         cumeq = cumsum(pl))

return(data)

}


# pl <- profit_loss(final_data, ramp_rate_MWH)

```

```{r}
# combine the strategy functions 
combined_strategy <- function(data, sd_window, sd_buy_percentile, sd_sell_percentile) {
  
  # Constants and initial conditions
  max_cycles <- 1.75
  hr <- 4
  ramp_rate_MWH <- 5 / 60 * 5 # 0.417 MW/5 minutes
  startSOC <- 0.001
  maxSOC <- 5 * hr # 5 MWh * 4hs = 20 MWh
  minSOC <- startSOC
  cycles_per <- max_cycles * 1 / 288 # Assuming 288 5-min intervals in a day

  # Apply the battery strategy
  strategy_data <- battery_strategy(
    data, sd_window, sd_buy_percentile, sd_sell_percentile,
    ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per
  )
  
  # Compute profit and loss
  pl_data <- profit_loss(strategy_data, ramp_rate_MWH)
  
  return(pl_data)
}


output_winter <- combined_strategy(winter, sd_window, sd_buy_percentile, sd_sell_percentile)
output_spring <- combined_strategy(spring, sd_window, sd_buy_percentile, sd_sell_percentile)
output_summer <- combined_strategy(summer, sd_window, sd_buy_percentile, sd_sell_percentile)
output_fall <- combined_strategy(fall, sd_window, sd_buy_percentile, sd_sell_percentile)

```


```{r}
# optimization function


optimize <- function(data, sd_window, sd_buy_percentile, sd_sell_percentile) {
  
 
  max_cycles <- 1.75
  hr <- 4
  ramp_rate_MWH <- 5 / 60 * 5 # 0.417 MW/5 minutes
  startSOC <- 0.001
  maxSOC <- 5 * hr # 5 MWh * 4hs = 20 MWh
  minSOC <- startSOC
  cycles_per <- max_cycles * 1 / 288 # Assuming 288 5-min intervals in a day

  # Apply the battery strategy
  strategy_data <- battery_strategy(
    data, sd_window, sd_buy_percentile, sd_sell_percentile,
    ramp_rate_MWH, startSOC, maxSOC, minSOC, max_cycles, cycles_per
  )
  
  # Compute profit and loss
  pl_data <- profit_loss(strategy_data, ramp_rate_MWH)
  
  total_cumeq <- pl_data %>% slice_tail(n = 1) %>% dplyr::select(cumeq)
  
  return(total_cumeq)
}

opt <- optimize(winter, sd_window, sd_buy_percentile, sd_sell_percentile)
```

```{r}
# Now I can start to optimize. Initialize parameter dfs

out <- expand.grid(
  sd_window = seq(from = 10, to = 30, by = 2),
  sd_buy_percentile = seq(from = 0.15, to = 0.45, by = 0.05),
  sd_sell_percentile = seq(from = 0.60, to = 0.90, by = 0.05)
)
```

```{r}
# optimization for winter
library(foreach)
library(doParallel)

n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
reswinter <- foreach(
  i = 1:nrow(out),
  .combine = "cbind",
  .packages = c(
    "tidyverse",
    "zoo",
    "TTR",
    "lubridate"
    )
) %dopar% {
  optimize(data = winter, out[i, "sd_window"], out[i, "sd_buy_percentile"], out["sd_sell_percentile"])
}
stopCluster(cl)

reswinter <- tibble::as_tibble(t(reswinter))
colnames(reswinter) <- names(opt)
outwinter <- cbind(out, reswinter)


```

```{r}
# optimization for summer
library(foreach)
library(doParallel)

n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
ressummer <- foreach(
  i = 1:nrow(out),
  .combine = "cbind",
  .packages = c(
    "tidyverse",
    "zoo",
    "TTR",
    "lubridate"
    )
) %dopar% {
  optimize(data = summer, out[i, "sd_window"], out[i, "sd_buy_percentile"], out["sd_sell_percentile"])
}
stopCluster(cl)

ressummer <- tibble::as_tibble(t(ressummer))
colnames(ressummer) <- names(opt)
outsummer <- cbind(out, ressummer)

```


```{r}
# optimization for fall
library(foreach)
library(doParallel)

n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
resfall <- foreach(
  i = 1:nrow(out),
  .combine = "cbind",
  .packages = c(
    "tidyverse",
    "zoo",
    "TTR",
    "lubridate"
    )
) %dopar% {
  optimize(data = fall, out[i, "sd_window"], out[i, "sd_buy_percentile"], out["sd_sell_percentile"])
}
stopCluster(cl)

resfall <- tibble::as_tibble(t(resfall))
colnames(resfall) <- names(opt)
outfall <- cbind(out, resfall)

```


```{r}
# optimization for spring
library(foreach)
library(doParallel)

n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
resspring <- foreach(
  i = 1:nrow(out),
  .combine = "cbind",
  .packages = c(
    "tidyverse",
    "zoo",
    "TTR",
    "lubridate"
    )
) %dopar% {
  optimize(data = spring, out[i, "sd_window"], out[i, "sd_buy_percentile"], out["sd_sell_percentile"])
}
stopCluster(cl)

resspring <- tibble::as_tibble(t(resspring))
colnames(resspring) <- names(opt)
outspring <- cbind(out, resspring)

```
## Optimization 

My parameters were optimized over 539 different combinations for each season, consisting of the following ranges: 

- Rolling SD window from 50 minutes to 2.5 hours. 
- Low vol buy quantiles set at a range of the 15th percentile to the 45th percentile.
- High vol sell quantiles set at a range of the 60th percentile to the 90th percentile.

The value that is being optimized for is strictly cumulative equity. The reasoning behind this approach is that the goal of this model is to align with the goal of CETF. They want these batteries to make money on the grid. The constraints of the battery inherently handle drawdowns and capital allocation and position sizes are handled by the maximum ramp rate of the battery. We can only buy as fast as the battery can charge, and can only sell as fast as the battery can discharge. This is implemented in the logic, and is why the only factor being analyzed during optimization is total profit throughout each season. 

To illustrate the results of optimization, I'll show the two parameters that had the most variable effect on cumulative equity, broken down by season. The below plots are 3 dimensional surface charts that show how cumulative equity changes across the rolling standard deviation (volatility) window and the buy signal quantile, with the optimal sell signal:    


```{r}
# pull optimal parameters
optspring <- outspring %>% arrange(desc(cumeq)) %>% slice_head(n = 1) %>% mutate(season = 'spring')
optwinter <- outwinter %>% arrange(desc(cumeq)) %>% slice_head(n = 1) %>% mutate(season = 'winter')
optfall <- outfall %>% arrange(desc(cumeq)) %>% slice_head(n = 1) %>% mutate(season = 'fall')
optsummer <- outsummer %>% arrange(desc(cumeq)) %>% slice_head(n = 1) %>% mutate(season = 'summer')

#spring surface
plotspring <- outspring %>% filter(sd_sell_percentile == optspring$sd_sell_percentile)

  
buy <- unique(plotspring$sd_buy_percentile)
window <- unique(plotspring$sd_window)
profit_spring <- plotspring %>% dplyr::select(sd_buy_percentile, sd_window, cumeq) %>% 
  pivot_wider(values_from = cumeq, names_from = sd_window) %>% dplyr::select(-1) %>%  as.matrix()
springplot <- plot_ly(x = ~window,
        y = ~buy,
        z = ~profit_spring) %>% add_surface()


# summer surface 
plotsummer <- outsummer %>% filter(sd_sell_percentile == optsummer$sd_sell_percentile)

  
buy <- unique(plotsummer$sd_buy_percentile)
window <- unique(plotsummer$sd_window)
profit_summer <- plotsummer %>% dplyr::select(sd_buy_percentile, sd_window, cumeq) %>% 
  pivot_wider(values_from = cumeq, names_from = sd_window) %>% dplyr::select(-1) %>%  as.matrix()
summerplot <- plot_ly(x = ~window,
        y = ~buy,
        z = ~profit_summer) %>% add_surface()



# fall surface
plotfall <- outfall %>% filter(sd_sell_percentile == optfall$sd_sell_percentile)

  
buy <- unique(plotfall$sd_buy_percentile)
window <- unique(plotfall$sd_window)
profit_fall <- plotfall %>% dplyr::select(sd_buy_percentile, sd_window, cumeq) %>% 
  pivot_wider(values_from = cumeq, names_from = sd_window) %>% dplyr::select(-1) %>%  as.matrix()
fallplot <- plot_ly(x = ~window,
        y = ~buy,
        z = ~profit_fall) %>% add_surface()
fallplot
# winter surface
plotwinter <- outwinter %>% filter(sd_sell_percentile == optwinter$sd_sell_percentile)

  
buy <- unique(plotwinter$sd_buy_percentile)
window <- unique(plotwinter$sd_window)
profit_winter <- plotwinter %>% dplyr::select(sd_buy_percentile, sd_window, cumeq) %>% 
  pivot_wider(values_from = cumeq, names_from = sd_window) %>% dplyr::select(-1) %>%  as.matrix()
winterplot <- plot_ly(x = ~window,
        y = ~buy,
        z = ~profit_winter) %>% add_surface()

springplot

summerplot

fallplot

winterplot

```

The profit variability is much more drastic for the high vol seasons (fall and winter) in comparison to the low vol seasons (spring and summer). This is to be expected as previously discussed. A volatility strategy is going to capture more profit and be much more variable and sensitive to parameter changes in periods of high volatility. 

Below is a table highlighting the optimal parameters, as seen on the surfaces above: 



```{r}

#combine to a dataframe to show as a table output
optimal_params <- bind_rows(optspring, optsummer, optfall, optwinter)

optimal_params %>% gt() %>%  tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 20, heading.title.font.size = 20)


  



```

Note that the sell quantile is the same for each season, asking for signals to be generated when volatility levels are above the 60% percentile. This is concerning, and will be discussed further in the learnings and findings portion of the document. 

Now that the optimal parameters have been discovered through iterating the model over different combinations, it is time to apply them to the model and run it on the testing data. 

```{r}
tdat <- all_state %>% mutate(Date = as.Date(DATETIME),
                            change = ACTUAL_PRICE - lag(ACTUAL_PRICE),
                      season = case_when(month %in% c(12, 1, 2) ~ 'Summer',
                          month %in% c(3, 4, 5) ~ 'Fall', 
                          month %in% c(6, 7, 8) ~ 'Winter',
                          month %in% c(9, 10, 11) ~ 'Spring', 
                          TRUE ~ 'na'))
tdat <- tdat %>% 
  filter(Date >= '2022-12-01', Date <= '2023-11-30') %>% 
  dplyr::select(DATETIME, ACTUAL_PRICE, change, season) 
twinter <- tdat %>% filter(season %in% 'Winter') %>% mutate(month = month(DATETIME))
tsummer <- tdat %>% filter(season %in% 'Summer') %>% mutate(month = month(DATETIME),
                                                           ord = case_when(month == 12 ~ 1,
                                                                           month == 1 ~ 2,
                                                                           TRUE ~ 3)) %>% arrange(ord) %>% select(-ord)
tfall <- tdat %>% filter(season %in% 'Fall') %>% mutate(month = month(DATETIME))
tspring <- tdat %>% filter(season %in% 'Spring') %>% mutate(month = month(DATETIME))
# test the model

test_winter <- combined_strategy(data = twinter, sd_window = 12, sd_buy_percentile = 0.30, sd_sell_percentile = 0.6)
test_spring <- combined_strategy(data = tspring, sd_window = 30, sd_buy_percentile = 0.40, sd_sell_percentile = 0.6)
test_summer <- combined_strategy(data = tsummer, sd_window = 24, sd_buy_percentile = 0.25, sd_sell_percentile = 0.75)
test_fall <- combined_strategy(data = tfall, sd_window = 10, sd_buy_percentile = 0.45, sd_sell_percentile = 0.6)






#combined eq plot
test_combined <- bind_rows(test_summer, test_fall, test_winter, test_spring) %>% arrange(DATETIME) %>% select(-cumeq) %>% mutate(cumeq = cumsum(pl)) 

final_plot <- test_combined %>% plot_ly(x = ~DATETIME, y = ~cumeq, type = 'scatter', mode = 'lines') %>% layout(title = 'Cumulative Profit Testing Data', xaxis = list(title = ''))
```


## Results

Below shows a plot of the results of the model for each season: 
```{r}
w <- test_winter %>% ggplot(aes(x = DATETIME, y = cumeq)) + geom_line(col = 'blue') + labs(x = '', title = 'Winter')
sp <- test_spring %>% ggplot(aes(x = DATETIME, y = cumeq)) + geom_line(col = 'orange') + labs(x = '', title = 'Spring')
s <- test_summer %>% ggplot(aes(x = DATETIME, y = cumeq)) + geom_line(col = 'red') + labs(x = '', title = 'Summer')
f <- test_fall %>% ggplot(aes(x = DATETIME, y = cumeq)) + geom_line(col = 'green') + labs(x = '', title = 'Fall')

cowplot::plot_grid(s, f, w, sp, ncol = 2)
```
The above plot highlights a few key factors: 

- I lose money in the low volatility seasons (spring and summer)
- I am making money in the high volatility seasons (fall and winter)

The profitability during high volatility seasons like fall and winter can be attributed to larger price swings which present more opportunities for arbitrage; the battery storage system can capitalize on these fluctuations by buying energy when prices are low and selling when they are high. Conversely, during low volatility seasons such as spring and summer, the price movements are less pronounced, resulting in fewer opportunities to exploit the differentials. 

Optimizing for each season individually may not fully mitigate this issue as my model relies solely on volatility to generate trading signals. If the underlying market conditions change states, for instance from high to low volatility, a model that doesn't adapt to these changes may not perform well.


Here is a plot that depicts total cumulative equity across the entire year:

```{r}

final_plot
```



This shows exactly what the prior plot was depicting; I make steady profit throughout the fall and winter, and lose it in the spring and summer. The model itself generates a total cumulative profit of just under $30,000 AUD. This is disheartening, and is nowhere near the amount of profit needed to cover the infrastructure costs of these batteries to make the investment worthwhile.

There is one positive that comes out of this though, which is highlighted in the chart below: 

```{r}
test_combined %>% group_by(month) %>%
  summarise(total_cycles = mean(abs(trade))) %>%
  ggplot(aes(x = month, y = total_cycles)) +
  geom_col(fill = 'blue')
```
Throughout the year, the average daily cycles are nowhere near reaching the capacity of 1.75 per day. Why is this a positive? Our previous model was very profitable, but did not exhaust the daily cycling capacity, meaning there is room to accommodate additional trading strategies without risking battery overuse. If We could find a way to implement additional signals in our previous model in relation to volatility that initiate trades to use up more of the capacity and make the model more profitable, this would add value, complexity, and ensure we are utilizing the battery to it's full potential. 


## Learnings and Findings

As explained above, my model did in fact answer the questions I laid out. The reason is the lack of model sophistication is impacting it's ability to properly detect the states of the market. Integrating a stochastic process into the trading strategy would provide a more dynamic model that recognizes and adjusts to changing market states. Stochastic models can incorporate random variables and probabilities that reflect the unpredictability of markets. By using stochastic calculus, you can model various market conditions and transition probabilities, allowing for more nuanced and responsive strategies. These could include mean-reverting processes during stable periods, and more trend-following or momentum-based processes during volatile periods, ensuring the strategy is tailored to the prevailing market state and can adapt as it evolves. My model does not have the ability to do this. 

My logic also appears to have a fatal flaw. The sell percentile signal, over the course of optimization, does not change. After hours of debugging and redesigning the logic, the same problem persisted. I know this can't possibly be attributable to market conditions, and is a fault in my programming. I just let the model pinpoint one SD sell signal, and looked at the variations in the other two variables (hence the surface diagrams).

The journey of refining my trading model has been both challenging and enlightening. Encountering the shortfalls of the current model—particularly its underperformance during low volatility seasons—has provided invaluable lessons on the complex dynamics of energy markets. Despite these hurdles, the deep-dive into the realm of stochastic models has been incredibly rewarding. It has not only broadened my understanding of market behavior but also fueled an interest in the predictive power of stochastic processes. This exploration into more advanced models, which account for the probabilistic nature of market states, has opened up new avenues for strategy development. It's clear that while the model may not have met all expectations, the research and iterative process of model improvement and what is used in practice has been time well spent, laying the groundwork for more robust and adaptive strategies in the future.

Electricity markets are incredibly complex, and this is the hardest problem I have ever tried to solve. I look forward to continuing to grow my abilities to the point where I can implement enterprise-level models in this space, and take what I have learned throughout this process to position myself well for success. My knowledge base of electricity grids and their financial implications, battery storage and utilization, and stochastic models has grown immensely. This problem will only continue to become more prominent in the future, and I am excited to say I have been able to dip my toes in it.  



